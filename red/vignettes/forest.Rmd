---
title: "Red-list African elephant dynamics (Forest)"
author: "AERL assessment team"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    fig_caption: yes
    toc: no
vignette: >
  %\VignetteIndexEntry{forest}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.path = 'fig/forest-', fig.width = 6, tidy = TRUE, tidy.opts = list(blank = TRUE, width.cutoff = 95), message = FALSE, warning = FALSE, collapse = TRUE, comment = "#>")

options(rmarkdown.html_vignette.check_title = FALSE)

suppressPackageStartupMessages(library(rstan))
suppressPackageStartupMessages(library(ggplot2))
options(mc.cores = parallel::detectCores())
suppressPackageStartupMessages(library(pander))
suppressPackageStartupMessages(library(plyr))
```

Load the package and extract stan model code
```{r}
library(red)

reg_ll <- stan_model(model_code = model_code("slope"), model_name = "log-linear")
```

Data are stored in the \code{redData} auxilliary package.

```{r}
data(forest, package = "redData")
```

## Log-linear model fit

Get initial values, run the model and examine posterior predictive fits to the data.
```{r}
# get initial values
ini <- with(forest, list(x0_log = rep(-1, sum(S)), 
            alpha0 = 0,
            alphaC = rep(0, C),
            alphaS = rep(0, sum(S)),
            sigma = 1, tau = c(0.1,0.1)))

reg_map <- optimizing(reg_ll, forest, init = function() ini, as_vector = FALSE)

reg_ini         <- list()
reg_ini$x0_log  <- reg_map$pars$x0_log
reg_ini$alpha0  <- reg_map$pars$alpha0
reg_ini$alphaC  <- reg_map$pars$alphaC
reg_ini$alphaS  <- reg_map$pars$alphaS
reg_ini$sigma   <- 1
reg_ini$tau     <- c(0.1,0.1)
```

```{r, results = 'hide'}
# MCMC sampling
reg_ll_out <- sampling(reg_ll, data = forest, init = function() reg_ini, chains = 2) 
```

```{r, echo = FALSE, fig.width=10, fig.height = 10}
gg <- traceplot(reg_ll_out, pars = c("parameter_summary", "error_summary", "output_summary"))
gg <- gg + facet_wrap(~parameter, scales = "free_y") + theme_bw(base_size = 12) + guides(col = "none")
gg <- gg + ggtitle("Trace summary statistics")
print(gg)
```

```{r, echo = FALSE, fig.width=10, fig.height=10}
dat$density_sim <- posterior(reg_ll_out, "y_sim", fun = "median")[[1]]

gg <- ggplot(dat) + 
    geom_point(aes(x = density, y = density_sim)) + 
    facet_wrap(~country) +
    labs(x = "Observed", y = "Predicted") + 
    geom_abline(intercept = 0, slope = 1) +
    theme_bw(base_size = 14) +
    scale_y_log10() + scale_x_log10() +
    ggtitle("Posterior prediction of survey density data")
print(gg)
```

# Trend estimate as the numbers weighted average decline

Estimation of the decline as an arithmetic average is presented to adhere with the IUCN Red List assessment criteria. The IUCN criteria stipulates that a numbers weighted average of the decline per site is required for Red List Assessments.

An absence of data estimating the numbers at each site at a common reference point in time makes this difficult (i.e., surveys were conducted erratically across Africa). As a result, to apply this method, the density derived from site level trends is converted into a numbers estimate using a static, assumed range area obtained from the modal survey area at that site. This survey area, as recorded in the data, is determined by a variety of factors, including expert opinion of range size and logistical practicalities of the survey. Therefore, it is unknown how closely the modal survey area represents the true range size for many sites. Furthermore, given that elephant ranges are known to have changed, a static range size is likely unrealistic for many of the sites. Converting density into numbers is therefore difficult and represents an approximation.

The numbers per site is obtained from model estimates of the log-density $x$ at site $i$ and time $t$, the observation error variance per site $\sigma^2$, the assumed range area size $a_i$ and the maximum likelihood estimate of the probability of a positive survey $\theta_i$:
$$
n_{i,t} = \theta_i \cdot \exp{(x_{i,t} + \sigma^2 / 2)}\cdot a_i
$$

The decline per site is then:
$$
\mathrm{decline\,per\,site} = 1 - \frac{n_{i,t}}{n_{i,t=0}}
$$

and the numbers weighted average of the decline is:
$$
\mathrm{decline} = \frac{\sum{(n_{i, t=0} \cdot \mathrm{decline\,per\,site}})}{\sum{n_{i, t=0}}}
$$

The posterior distribution of the average decline is:
```{r}
dfr <- posterior(reg_ll_out, pars = "decline", melt = TRUE)[[1]]

gg <- ggplot(dfr) + 
  geom_histogram(aes(value, ..density..), binwidth = 0.1) +
  theme_bw(base_size = 20) %+replace% theme(axis.text.y = element_blank()) +
  scale_x_continuous(limits = c(-0.5, 1.2)) +
  labs(x = "Average Decline", y = "") +
  geom_vline(xintercept = c(0.2, 0.5, 0.8, 1.0), col = "red")
print(gg)
```

We can also plot the decline per site ($1 - \frac{n_{i,t}}{n_{i,t=0}}$):
```{r, echo = TRUE, fig.height=10}
gg <- plot(reg_ll_out, pars = "decline_site") + theme(axis.text.y = element_blank()) + ggtitle("Numbers decline per site")
gg <- gg + geom_vline(xintercept = posterior(reg_ll_out, "decline", fun = "median")[[1]], col = "blue", size = 2, alpha = 0.6)
print(gg)
```

It is requested that this assessment includes a table of the declines per site, in this case taking median values per site:
```{r, results = "asis"}
year_N0 <- 1984

nit <- 2000

zones <- row.names(forest$survey_area)
  
lookup <- dlply(dat, .(zone), summarize, as.character(unique(country)))
lookup <- lapply(lookup, function(x) x[,1])

country_lookup <- function(zone) {
    
    lookup[[as.character(zone)]]
}


N0 <- posterior(reg_ll_out, pars = "N0", melt = TRUE, fun = "median", dim.names = list(list(iter = 1:nit, zone = zones)))[[1]]
NT <- posterior(reg_ll_out, pars = "NT", melt = TRUE, fun = "median", dim.names = list(list(iter = 1:nit, zone = zones)))[[1]]
ds <- posterior(reg_ll_out, pars = "decline_site", melt = TRUE, fun = "median", dim.names = list(list(iter = 1:nit, zone = zones)))[[1]]

dfr <- cbind(N0, NT, ds)
colnames(dfr) <- c(paste0("N_", year_N0), "N_2015", "decline")
dfr$zone <- rownames(dfr)
dfr$country <- vapply(dfr$zone, country_lookup, character(1))

dfr2 <- data.frame(range_area = forest$range_area, zone = zones)

dfr <- merge(dfr, dfr2, by = "zone")

dfr <- dfr[order(dfr$country),]
row.names(dfr) <- NULL
pandoc.table(dfr[, c(5, 1, 2, 3, 4, 6)], style = "rmarkdown", split.tables = Inf)
```

Check internal consistency of table:
```{r, echo = FALSE}
plot(1 - N_2015 / N_1984 ~ decline, dfr)
abline(0,1)
```

From the table we can extract a numbers weighted average of the median decline per site, which differs markedly from the posterior distributions given above:
```{r}
with(dfr, sum(N_1984 * decline) / sum(N_1984))
```

Finally we can tabulate the results:
```{r, results = "asis"}
dfr <- posterior(reg_ll_out, pars = "decline", melt = TRUE)[[1]]
  
ff <- function(x) paste0(round(median(x, na.rm = TRUE), 2), " (", round(quantile(x, 0.025, na.rm = TRUE), 2), " - ", round(quantile(x, 0.975, na.rm = TRUE), 2), ")")

out <-  with(dfr, data.frame(
             decline = ff(value), 
             critical   = round(mean(value > 0.8),2), 
             endangered = round(mean(value <= 0.8 & value > 0.5),2), 
             vulnerable = round(mean(value <= 0.5 & value > 0.2),2), 
             not_threatened = round(mean(value <= 0.2),2)))
pandoc.table(out, style = "rmarkdown")
```

