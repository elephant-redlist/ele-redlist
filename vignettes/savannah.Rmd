---
title: "Red-list African elephant dynamics (Savannah)"
author: "AERL assessment team"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    fig_caption: yes
    toc: no
vignette: >
  %\VignetteIndexEntry{savannah}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.path = 'fig/savannah-', fig.width = 6, tidy = TRUE, tidy.opts = list(blank = TRUE, width.cutoff = 95), message = FALSE, warning = FALSE, collapse = TRUE, comment = "#>")

options(rmarkdown.html_vignette.check_title = FALSE)

suppressPackageStartupMessages(library(rstan))
options(mc.cores = parallel::detectCores())
suppressPackageStartupMessages(library(pander))
suppressPackageStartupMessages(library(plyr))
```

Load the package and extract stan model code
```{r}
library(red)

reg_ll <- stan_model(model_code = model_code("slope"), model_name = "log-linear")
reg_lp <- stan_model(model_code = model_code("poly"),  model_name = "log-polynomial")
```

Data are stored in the \code{redData} auxilliary package.

```{r}
data(savannah, package = "redData")
```

## Log-linear model fit

Get initial values, run the model and examine posterior predictive fits to the data.
```{r}
# get initial values
ini <- with(savannah, list(x0_log = rep(-1, sum(S)), 
            alpha0 = 0,
            alphaC = rep(0, C),
            alphaS = rep(0, sum(S)),
            sigma = 1, tau = c(0.1,0.1)))

reg_map <- optimizing(reg_ll, savannah, init = function() ini, as_vector = FALSE)

reg_ini         <- list()
reg_ini$x0_log  <- reg_map$pars$x0_log
reg_ini$alpha0  <- reg_map$pars$alpha0
reg_ini$alphaC  <- reg_map$pars$alphaC
reg_ini$alphaS  <- reg_map$pars$alphaS
reg_ini$sigma   <- 1
reg_ini$tau     <- c(0.1,0.1)
```

```{r, results = 'hide'}
# MCMC sampling
reg_ll_out <- sampling(reg_ll, data = savannah, init = function() reg_ini, chains = 2) 
```

```{r, echo = FALSE, fig.width=10, fig.height=10}
gg <- traceplot(reg_ll_out, pars = c("parameter_summary", "error_summary", "output_summary"))
gg <- gg + facet_wrap(~parameter, scales = "free_y") + theme_bw(base_size = 12) + guides(col = "none")
gg <- gg + ggtitle("Trace summary statistics")
print(gg)
```

```{r, echo = TRUE, fig.width=10, fig.height=10}
dat$density_sim <- posterior(reg_ll_out, "y_sim", fun = "median")[[1]]

gg <- ggplot(dat) + 
    geom_point(aes(x = density, y = density_sim)) + 
    facet_wrap(~country) +
    labs(x = "Observed", y = "Predicted") + 
    geom_abline(intercept = 0, slope = 1) +
    theme_bw(base_size = 14) +
    scale_y_log10() + scale_x_log10() +
    ggtitle("Posterior prediction of survey density data")
print(gg)
```

## Log-polynomial model fit
Get initial values, run the model and examine posterior predictive fits to the data.
```{r}
# get initial values
ini <- with(savannah, list(x0_log = rep(-1, sum(S)), 
            alpha0 = 0,
            alphaC = rep(0, C),
            alphaS = rep(0, sum(S)),
            beta0 = 0,
            betaC = rep(0, C),
            betaS = rep(0, sum(S)),
            sigma = 1, tau = c(0.1,0.1,0.1,0.1)))

reg_map <- optimizing(reg_lp, savannah, init = function() ini, as_vector = FALSE)

reg_ini         <- list()
reg_ini$x0_log  <- reg_map$pars$x0_log
reg_ini$alpha0  <- reg_map$pars$alpha0
reg_ini$alphaC  <- reg_map$pars$alphaC
reg_ini$alphaS  <- reg_map$pars$alphaS
reg_ini$beta0   <- reg_map$pars$beta0
reg_ini$betaC   <- reg_map$pars$betaC
reg_ini$betaS   <- reg_map$pars$betaS
reg_ini$sigma   <- 1
reg_ini$tau     <- c(0.1,0.1,0.1,0.1)
```

```{r, results = 'hide'}
# MCMC sampling
reg_lp_out <- sampling(reg_lp, data = savannah, init = function() reg_ini, chains = 2) 
```

```{r, echo = FALSE, fig.width=10, fig.height=10}
gg <- traceplot(reg_lp_out, pars = c("parameter_summary", "error_summary", "output_summary"))
gg <- gg + facet_wrap(~parameter, scales = "free_y") + theme_bw(base_size = 12) + guides(col = "none")
gg <- gg + ggtitle("Trace summary statistics")
print(gg)
```

```{r, echo = TRUE, fig.width=10, fig.height=10}
dat$density_sim <- posterior(reg_lp_out, "y_sim", fun = "median")[[1]]

gg <- ggplot(dat) + 
    geom_point(aes(x = density, y = density_sim)) + 
    facet_wrap(~country) +
    labs(x = "Observed", y = "Predicted") + 
    geom_abline(intercept = 0, slope = 1) +
    theme_bw(base_size = 14) +
    scale_y_log10() + scale_x_log10() +
    ggtitle("Posterior prediction of survey density data")
print(gg)
```

## Compare predictions

Predictive abilities of the models are compared using the mean absolute prediction error:
$$
MPE = \frac{1}{n}\sum{\frac{|\hat{y}_i - y_i|}{y_i}}
$$
where $y_i$ is the observation and $\hat{y}_i$ is the posterior predicted value. Because the MPE distributions are similar between the two model fits we conclude that the polynomial model does not provide any improvement in terms of predictive ability, and select the log-linear model as the most parsimonious representation of the data. This is supported by visual inspection of the trace diagnostics, which show slower convergence for the more complicated (log-polynomial) model.

```{r}
dfr_ll <- posterior(reg_ll_out, pars = "MPE", melt = TRUE)[[1]]
dfr_lp <- posterior(reg_lp_out, pars = "MPE", melt = TRUE)[[1]]

dfr <- merge(dfr_ll, dfr_lp, by = "iterations", suffixes = c(".ll", ".lp"))
dfr <- reshape2::melt(dfr, id.vars = "iterations")

gg <- ggplot(dfr) +
    geom_density(aes(value, fill = variable), alpha = 0.3) + labs(x = "Mean absolute prediction error", y = "") +
    theme_bw(base_size = 16)
print(gg)
```

# Trend estimate as the numbers weighted average decline

Estimation of the decline as an arithmetic average is presented to adhere with the IUCN Red List assessment criteria. The IUCN criteria stipulates that a numbers weighted average of the decline per site is required for Red List Assessments.

An absence of data estimating the numbers at each site at a common reference point in time makes this difficult (i.e., surveys were conducted erratically across Africa). As a result, to apply this method, the density derived from site level trends is converted into a numbers estimate using a static, assumed range area obtained from the modal survey area at that site. This survey area, as recorded in the data, is determined by a variety of factors, including expert opinion of range size and logistical practicalities of the survey. Therefore, it is unknown how closely the modal survey area represents the true range size for many sites. Furthermore, given that elephant ranges are known to have changed, a static range size is likely unrealistic for many of the sites. Converting density into numbers is therefore difficult and represents an approximation.

The numbers per site is obtained from model estimates of the log-density $x$ at site $i$ and time $t$, the observation error variance per site $\sigma^2$, the assumed range area size $a_i$ and the maximum likelihood estimate of the probability of a positive survey $\theta_i$ (typically close to one):
$$
n_{i,t} = \theta_i \cdot \exp{(x_{i,t} + \sigma^2 / 2)}\cdot a_i
$$

The decline per site is then:
$$
\mathrm{decline\,per\,site} = 1 - \frac{n_{i,t}}{n_{i,t=0}}
$$

and the numbers weighted average of the decline is:
$$
\mathrm{decline} = \frac{\sum{(n_{i, t=0} \cdot \mathrm{decline\,per\,site}})}{\sum{n_{i, t=0}}}
$$

The posterior distribution of the average decline is:
```{r}
dfr <- posterior(reg_ll_out, pars = "decline", melt = TRUE)[[1]]

gg <- ggplot(dfr) + 
  geom_histogram(aes(value, ..density..), binwidth = 0.1) +
  theme_bw(base_size = 20) %+replace% theme(axis.text.y = element_blank()) +
  scale_x_continuous(limits = c(-0.5, 1.2)) +
  labs(x = "Average Decline", y = "") +
  geom_vline(xintercept = c(0.2, 0.5, 0.8, 1.0), col = "red")
print(gg)
```

We can also plot the decline per site ($1 - \frac{n_{i,t}}{n_{i,t=0}}$):
```{r, echo = TRUE, fig.height=10}
gg <- plot(reg_ll_out, pars = "decline_site") + theme(axis.text.y = element_blank()) + ggtitle("Numbers decline per site")
gg <- gg + geom_vline(xintercept = posterior(reg_ll_out, "decline", fun = "median")[[1]], col = "blue", size = 2, alpha = 0.6)
print(gg)
```

It is requested that this assessment includes a table of the declines per site, in this case taking median values per site:
```{r, results = "asis"}
year_N0 <- 1965

nit <- 2000

zones <- row.names(savannah$survey_area)
  
lookup <- dlply(dat, .(zone), summarize, as.character(unique(country)))
lookup <- lapply(lookup, function(x) x[,1])

country_lookup <- function(zone) {
    
    lookup[[as.character(zone)]]
}


N0 <- posterior(reg_ll_out, pars = "N0", melt = TRUE, fun = "median", dim.names = list(list(iter = 1:nit, zone = zones)))[[1]]
NT <- posterior(reg_ll_out, pars = "NT", melt = TRUE, fun = "median", dim.names = list(list(iter = 1:nit, zone = zones)))[[1]]
ds <- posterior(reg_ll_out, pars = "decline_site", melt = TRUE, fun = "median", dim.names = list(list(iter = 1:nit, zone = zones)))[[1]]

dfr <- cbind(N0, NT, ds)
colnames(dfr) <- c(paste0("N_", year_N0), "N_2015", "decline")
dfr$zone <- rownames(dfr)
dfr$country <- vapply(dfr$zone, country_lookup, character(1))

dfr2 <- data.frame(range_area = savannah$range_area, zone = zones)

dfr <- merge(dfr, dfr2, by = "zone")

dfr <- dfr[order(dfr$country),]
row.names(dfr) <- NULL
pandoc.table(dfr[, c(5, 1, 2, 3, 4, 6)], style = "rmarkdown", split.tables = Inf)
```

Check internal consistency of table:
```{r, echo = FALSE}
plot(1 - N_2015 / N_1965 ~ decline, dfr)
abline(0,1)
```

From the table we can extract a numbers weighted average of the median decline per site, which differs markedly from the posterior distributions given above:
```{r}
with(dfr, sum(N_1965 * decline) / sum(N_1965))
```

Finally we tabulate the results:
```{r, results = "asis"}
dfr <- posterior(reg_ll_out, pars = "decline", melt = TRUE)[[1]]
  
ff <- function(x) paste0(round(median(x, na.rm = TRUE), 2), " (", round(quantile(x, 0.025, na.rm = TRUE), 2), " - ", round(quantile(x, 0.975, na.rm = TRUE), 2), ")")

out <-  with(dfr, data.frame(
             decline = ff(value), 
             critical   = round(mean(value > 0.8),2), 
             endangered = round(mean(value <= 0.8 & value > 0.5),2), 
             vulnerable = round(mean(value <= 0.5 & value > 0.2),2), 
             not_threatened = round(mean(value <= 0.2),2)))
pandoc.table(out, style = "rmarkdown")
```

